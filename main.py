<<<<<<< HEAD
from dotenv import load_dotenv
import os

load_dotenv()

from datetime import datetime
import streamlit as st
from uuid import uuid4
from common.agent_module import run_agent
from common.config import should_continue
from common.tool_module import tools, jeju_recommendation
from langgraph.prebuilt.tool_executor import ToolExecutor


# ToolExecutor ìƒì„±
tool_executor = ToolExecutor(tools)

# URLê³¼ ì œëª©ì„ ì¶”ì¶œí•˜ì—¬ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
def display_sites(site_data):
    st.write("### ì¶”ì²œ ë¸”ë¡œê·¸ ëª©ë¡:")
    for site in site_data:
        url = site['url']
        title = site['content'].split(":")[0]  # ì œëª©ë§Œ ì¶”ì¶œ
        st.markdown(f"- [{title}]({url})", unsafe_allow_html=True)

# Streamlit ì„¸ì…˜ ì´ˆê¸°í™”
if "chat_sessions" not in st.session_state:
    st.session_state.chat_sessions = {}
if "current_session_id" not in st.session_state:
    st.session_state.current_session_id = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Streamlit ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.title("ðŸŠ ì œì£¼ë„ ì—¬í–‰ ê°€ì´ë“œ")
st.sidebar.write("ì±„íŒ…ë°© ëª©ë¡:")

for session_id, session_data in st.session_state.chat_sessions.items():
    if st.sidebar.button(session_data["title"], key=session_id):
        st.session_state.current_session_id = session_id
        st.session_state.chat_history = session_data["messages"]

# ìƒˆ ì±„íŒ…ë°© ì‹œìž‘ ë²„íŠ¼
if st.sidebar.button("ìƒˆ ì±„íŒ…ë°© ì‹œìž‘"):
    new_session_id = str(uuid4())
    st.session_state.chat_sessions[new_session_id] = {
        "title": f"ì±„íŒ…ë°© {len(st.session_state.chat_sessions) + 1}",
        "messages": [],
        "created_at": datetime.now()
    }
    st.session_state.current_session_id = new_session_id
    st.session_state.chat_history = []

# í˜„ìž¬ ì„ íƒëœ ì±„íŒ…ë°© ê°€ì ¸ì˜¤ê¸°
current_session = st.session_state.chat_sessions.get(st.session_state.current_session_id)

# ë©”ì¸ í™”ë©´ ì„¤ì •
st.title("ðŸŠ ì œì£¼ë„ ì—¬í–‰ ê°€ì´ë“œ ðŸ›«")

# ì±„íŒ…ë°©ì´ ì„ íƒë˜ì—ˆì„ ê²½ìš°
if current_session:
    st.write(f"**{current_session['title']}**")

    # ì €ìž¥ëœ ë©”ì‹œì§€ ì¶œë ¥ (ì±„íŒ… ê¸°ë¡ì´ ì œëŒ€ë¡œ ìœ ì§€ë˜ë„ë¡)
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ìž ìž…ë ¥ ë°›ê¸°
    prompt = st.chat_input("ì œì£¼ë„ ì—¬í–‰ ê´€ë ¨ ì§ˆë¬¸ì„ ìž…ë ¥í•´ì£¼ì„¸ìš” ~!")

    if prompt:
        # ì‚¬ìš©ìž ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ì— ì¶”ê°€í•˜ê³  í™”ë©´ì— í‘œì‹œ
        user_message = {
            "role": "user",
            "content": prompt,
            "timestamp": datetime.now()
        }
        st.session_state.chat_history.append(user_message)
        current_session["messages"].append(user_message)

        # ì‚¬ìš©ìž ë©”ì‹œì§€ í™”ë©´ì— í‘œì‹œ
        with st.chat_message("user"):
            st.markdown(prompt)

        # "ì œì£¼"ê°€ í¬í•¨ëœ ê²½ìš°ì—ë§Œ `jeju_recommendation`ì„ í˜¸ì¶œí•˜ì—¬ ì™¸ë¶€ ì§€ì—­ ì§ˆë¬¸ ì°¨ë‹¨
        if "ì œì£¼" in prompt:
            recommendation_response = jeju_recommendation(prompt)
            
            # ì œì£¼ë„ ì™¸ ì§€ì—­ ìš”ì²­ ì‹œ ì•ˆë‚´ ë©”ì‹œì§€ í‘œì‹œ ë° ì—ì´ì „íŠ¸ ì‹¤í–‰ ê±´ë„ˆë›°ê¸°
            if recommendation_response != "ì œì£¼ë„ ì—¬í–‰ ì¶”ì²œ ì±—ë´‡ìž…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?":
                with st.chat_message("assistant"):
                    st.markdown(recommendation_response)
                
                # ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ë° UIì— ì¶”ê°€
                assistant_message = {
                    "role": "assistant",
                    "content": recommendation_response,
                    "timestamp": datetime.now()
                }
                st.session_state.chat_history.append(assistant_message)
                current_session["messages"].append(assistant_message)
            else:
                # ì œì£¼ë„ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì´ë¯€ë¡œ ì—ì´ì „íŠ¸ ì‹¤í–‰
                agent_input = {"input": prompt, "chat_history": st.session_state.chat_history, "intermediate_steps": []}
                agent_outcome = run_agent(agent_input)

                # ì—ì´ì „íŠ¸ ê²°ê³¼ ì²˜ë¦¬
                if should_continue(agent_outcome) == "end":
                    bot_response = agent_outcome["agent_outcome"].return_values.get("output", "ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                    
                    if isinstance(bot_response, list) and all('url' in item and 'content' in item for item in bot_response):
                        display_sites(bot_response)
                    else:
                        with st.chat_message("assistant"):
                            st.markdown(bot_response)
                else:
                    agent_action = agent_outcome["agent_outcome"]
                    output = tool_executor.invoke(agent_action)

                    if isinstance(output, list) and all('url' in item and 'content' in item for item in output):
                        display_sites(output)
                        bot_response = "ì¶”ì²œ ë¸”ë¡œê·¸ ëª©ë¡ì„ ë³´ì—¬ë“œë ¸ìŠµë‹ˆë‹¤."
                    else:
                        bot_response = str(output)
                        with st.chat_message("assistant"):
                            st.markdown(bot_response)

                # ì±—ë´‡ ì‘ë‹µì„ ì„¸ì…˜ ë° UIì— ì¶”ê°€
                assistant_message = {
                    "role": "assistant",
                    "content": bot_response,
                    "timestamp": datetime.now()
                }
                st.session_state.chat_history.append(assistant_message)
                current_session["messages"].append(assistant_message)

        else:
            # ì œì£¼ì™€ ê´€ë ¨ë˜ì§€ ì•Šì€ ìž…ë ¥ì€ ì—ì´ì „íŠ¸ë¥¼ ë°”ë¡œ ì‹¤í–‰
            agent_input = {"input": prompt, "chat_history": st.session_state.chat_history, "intermediate_steps": []}
            agent_outcome = run_agent(agent_input)

            # ì—ì´ì „íŠ¸ ê²°ê³¼ ì²˜ë¦¬
            if should_continue(agent_outcome) == "end":
                bot_response = agent_outcome["agent_outcome"].return_values.get("output", "ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                
                if isinstance(bot_response, list) and all('url' in item and 'content' in item for item in bot_response):
                    display_sites(bot_response)
                else:
                    with st.chat_message("assistant"):
                        st.markdown(bot_response)
            else:
                agent_action = agent_outcome["agent_outcome"]
                output = tool_executor.invoke(agent_action)

                if isinstance(output, list) and all('url' in item and 'content' in item for item in output):
                    display_sites(output)
                    bot_response = "ì¶”ì²œ ë¸”ë¡œê·¸ ëª©ë¡ì„ ë³´ì—¬ë“œë ¸ìŠµë‹ˆë‹¤."
                else:
                    bot_response = str(output)
                    with st.chat_message("assistant"):
                        st.markdown(bot_response)

            # ì±—ë´‡ ì‘ë‹µì„ ì„¸ì…˜ ë° UIì— ì¶”ê°€
            assistant_message = {
                "role": "assistant",
                "content": bot_response,
                "timestamp": datetime.now()
            }
            st.session_state.chat_history.append(assistant_message)
            current_session["messages"].append(assistant_message)

else:
    st.write("ì•ˆë…•í•˜ì„¸ìš” â˜ºï¸ ì œì£¼ë„ ì—¬í–‰ ê³„íš ì¤‘ì´ì‹ ê°€ìš”? \n\nì™¼ìª½ì—ì„œ ì±„íŒ…ë°©ì„ ì„ íƒí•˜ê±°ë‚˜ ìƒˆë¡œ ì‹œìž‘í•˜ì„¸ìš”.")
=======
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.chains import RetrievalQA
import streamlit as st
import os
import tempfile
from model import pdf_to_document
# Load .env 
from dotenv import load_dotenv
load_dotenv()

# ì œëª©
st.title("ChatPDF")
st.write("---")
# íŒŒì¼ ì—…ë¡œë“œ
uploaded_file = st.file_uploader("Choose a file")
st.write("---")

#ì—…ë¡œë“œ ë˜ë©´ ë™ìž‘í•˜ëŠ” ì½”ë“œ
if uploaded_file is not None:
    pages = pdf_to_document(uploaded_file)

    #Split
    text_splitter = RecursiveCharacterTextSplitter(
        # Set a really small chunk size, just to show.
        chunk_size = 1000,
        chunk_overlap  = 20,
        length_function = len,
        is_separator_regex = False,
    )
    texts = text_splitter.split_documents(pages)

    #Embedding
    embeddings_model = OpenAIEmbeddings()

    # load it into Chroma
    db = Chroma.from_documents(texts, embeddings_model)

    #Question
    st.header("PDFì—ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”!!")
    question = st.text_input('ì§ˆë¬¸ì„ ìž…ë ¥í•˜ì„¸ìš”')

    if st.button('ì§ˆë¬¸í•˜ê¸°'):
        with st.spinner('Wait for it...'):
            llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
            qa_chain = RetrievalQA.from_chain_type(llm,retriever=db.as_retriever(), return_source_documents=True)
            result = qa_chain.invoke({"query": question})
            print(result['source_documents'])
            st.write(result["result"])



>>>>>>> a762fa9f9050eb150ae4f02cee0766e0b7025e1c
