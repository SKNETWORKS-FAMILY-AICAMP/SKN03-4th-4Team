<<<<<<< HEAD
=======
<<<<<<< HEAD
from dotenv import load_dotenv
import os

load_dotenv()

from datetime import datetime
import streamlit as st
from uuid import uuid4
from common.agent_module import run_agent
from common.config import should_continue
from common.tool_module import tools, jeju_recommendation
from langgraph.prebuilt.tool_executor import ToolExecutor


# ToolExecutor ìƒì„±
tool_executor = ToolExecutor(tools)

# URLê³¼ ì œëª©ì„ ì¶”ì¶œí•˜ì—¬ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
def display_sites(site_data):
    st.write("### ì¶”ì²œ ë¸”ë¡œê·¸ ëª©ë¡:")
    for site in site_data:
        url = site['url']
        title = site['content'].split(":")[0]  # ì œëª©ë§Œ ì¶”ì¶œ
        st.markdown(f"- [{title}]({url})", unsafe_allow_html=True)

# Streamlit ì„¸ì…˜ ì´ˆê¸°í™”
if "chat_sessions" not in st.session_state:
    st.session_state.chat_sessions = {}
if "current_session_id" not in st.session_state:
    st.session_state.current_session_id = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Streamlit ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.title("ğŸŠ ì œì£¼ë„ ì—¬í–‰ ê°€ì´ë“œ")
st.sidebar.write("ì±„íŒ…ë°© ëª©ë¡:")

for session_id, session_data in st.session_state.chat_sessions.items():
    if st.sidebar.button(session_data["title"], key=session_id):
        st.session_state.current_session_id = session_id
        st.session_state.chat_history = session_data["messages"]

# ìƒˆ ì±„íŒ…ë°© ì‹œì‘ ë²„íŠ¼
if st.sidebar.button("ìƒˆ ì±„íŒ…ë°© ì‹œì‘"):
    new_session_id = str(uuid4())
    st.session_state.chat_sessions[new_session_id] = {
        "title": f"ì±„íŒ…ë°© {len(st.session_state.chat_sessions) + 1}",
        "messages": [],
        "created_at": datetime.now()
    }
    st.session_state.current_session_id = new_session_id
    st.session_state.chat_history = []

# í˜„ì¬ ì„ íƒëœ ì±„íŒ…ë°© ê°€ì ¸ì˜¤ê¸°
current_session = st.session_state.chat_sessions.get(st.session_state.current_session_id)

# ë©”ì¸ í™”ë©´ ì„¤ì •
st.title("ğŸŠ ì œì£¼ë„ ì—¬í–‰ ê°€ì´ë“œ ğŸ›«")

# ì±„íŒ…ë°©ì´ ì„ íƒë˜ì—ˆì„ ê²½ìš°
if current_session:
    st.write(f"**{current_session['title']}**")

    # ì €ì¥ëœ ë©”ì‹œì§€ ì¶œë ¥ (ì±„íŒ… ê¸°ë¡ì´ ì œëŒ€ë¡œ ìœ ì§€ë˜ë„ë¡)
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    prompt = st.chat_input("ì œì£¼ë„ ì—¬í–‰ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” ~!")

    if prompt:
        # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ì— ì¶”ê°€í•˜ê³  í™”ë©´ì— í‘œì‹œ
        user_message = {
            "role": "user",
            "content": prompt,
            "timestamp": datetime.now()
        }
        st.session_state.chat_history.append(user_message)
        current_session["messages"].append(user_message)

        # ì‚¬ìš©ì ë©”ì‹œì§€ í™”ë©´ì— í‘œì‹œ
        with st.chat_message("user"):
            st.markdown(prompt)

        # "ì œì£¼"ê°€ í¬í•¨ëœ ê²½ìš°ì—ë§Œ `jeju_recommendation`ì„ í˜¸ì¶œí•˜ì—¬ ì™¸ë¶€ ì§€ì—­ ì§ˆë¬¸ ì°¨ë‹¨
        if "ì œì£¼" in prompt:
            recommendation_response = jeju_recommendation(prompt)
            
            # ì œì£¼ë„ ì™¸ ì§€ì—­ ìš”ì²­ ì‹œ ì•ˆë‚´ ë©”ì‹œì§€ í‘œì‹œ ë° ì—ì´ì „íŠ¸ ì‹¤í–‰ ê±´ë„ˆë›°ê¸°
            if recommendation_response != "ì œì£¼ë„ ì—¬í–‰ ì¶”ì²œ ì±—ë´‡ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?":
                with st.chat_message("assistant"):
                    st.markdown(recommendation_response)
                
                # ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ë° UIì— ì¶”ê°€
                assistant_message = {
                    "role": "assistant",
                    "content": recommendation_response,
                    "timestamp": datetime.now()
                }
                st.session_state.chat_history.append(assistant_message)
                current_session["messages"].append(assistant_message)
            else:
                # ì œì£¼ë„ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì´ë¯€ë¡œ ì—ì´ì „íŠ¸ ì‹¤í–‰
                agent_input = {"input": prompt, "chat_history": st.session_state.chat_history, "intermediate_steps": []}
                agent_outcome = run_agent(agent_input)

                # ì—ì´ì „íŠ¸ ê²°ê³¼ ì²˜ë¦¬
                if should_continue(agent_outcome) == "end":
                    bot_response = agent_outcome["agent_outcome"].return_values.get("output", "ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                    
                    if isinstance(bot_response, list) and all('url' in item and 'content' in item for item in bot_response):
                        display_sites(bot_response)
                    else:
                        with st.chat_message("assistant"):
                            st.markdown(bot_response)
                else:
                    agent_action = agent_outcome["agent_outcome"]
                    output = tool_executor.invoke(agent_action)

                    if isinstance(output, list) and all('url' in item and 'content' in item for item in output):
                        display_sites(output)
                        bot_response = "ì¶”ì²œ ë¸”ë¡œê·¸ ëª©ë¡ì„ ë³´ì—¬ë“œë ¸ìŠµë‹ˆë‹¤."
                    else:
                        bot_response = str(output)
                        with st.chat_message("assistant"):
                            st.markdown(bot_response)

                # ì±—ë´‡ ì‘ë‹µì„ ì„¸ì…˜ ë° UIì— ì¶”ê°€
                assistant_message = {
                    "role": "assistant",
                    "content": bot_response,
                    "timestamp": datetime.now()
                }
                st.session_state.chat_history.append(assistant_message)
                current_session["messages"].append(assistant_message)

        else:
            # ì œì£¼ì™€ ê´€ë ¨ë˜ì§€ ì•Šì€ ì…ë ¥ì€ ì—ì´ì „íŠ¸ë¥¼ ë°”ë¡œ ì‹¤í–‰
            agent_input = {"input": prompt, "chat_history": st.session_state.chat_history, "intermediate_steps": []}
            agent_outcome = run_agent(agent_input)

            # ì—ì´ì „íŠ¸ ê²°ê³¼ ì²˜ë¦¬
            if should_continue(agent_outcome) == "end":
                bot_response = agent_outcome["agent_outcome"].return_values.get("output", "ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                
                if isinstance(bot_response, list) and all('url' in item and 'content' in item for item in bot_response):
                    display_sites(bot_response)
                else:
                    with st.chat_message("assistant"):
                        st.markdown(bot_response)
            else:
                agent_action = agent_outcome["agent_outcome"]
                output = tool_executor.invoke(agent_action)

                if isinstance(output, list) and all('url' in item and 'content' in item for item in output):
                    display_sites(output)
                    bot_response = "ì¶”ì²œ ë¸”ë¡œê·¸ ëª©ë¡ì„ ë³´ì—¬ë“œë ¸ìŠµë‹ˆë‹¤."
                else:
                    bot_response = str(output)
                    with st.chat_message("assistant"):
                        st.markdown(bot_response)

            # ì±—ë´‡ ì‘ë‹µì„ ì„¸ì…˜ ë° UIì— ì¶”ê°€
            assistant_message = {
                "role": "assistant",
                "content": bot_response,
                "timestamp": datetime.now()
            }
            st.session_state.chat_history.append(assistant_message)
            current_session["messages"].append(assistant_message)

else:
    st.write("ì•ˆë…•í•˜ì„¸ìš” â˜ºï¸ ì œì£¼ë„ ì—¬í–‰ ê³„íš ì¤‘ì´ì‹ ê°€ìš”? \n\nì™¼ìª½ì—ì„œ ì±„íŒ…ë°©ì„ ì„ íƒí•˜ê±°ë‚˜ ìƒˆë¡œ ì‹œì‘í•˜ì„¸ìš”.")
=======
>>>>>>> dev
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
<<<<<<< HEAD
from chromadb.config import Settings
=======
>>>>>>> dev
from langchain_openai import ChatOpenAI
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.chains import RetrievalQA
import streamlit as st
import os
import tempfile
<<<<<<< HEAD
=======
from model import pdf_to_document
# Load .env 
>>>>>>> dev
from dotenv import load_dotenv
load_dotenv()

# ì œëª©
st.title("ChatPDF")
st.write("---")
<<<<<<< HEAD

=======
>>>>>>> dev
# íŒŒì¼ ì—…ë¡œë“œ
uploaded_file = st.file_uploader("Choose a file")
st.write("---")

<<<<<<< HEAD
# PDF íŒŒì¼ì„ ë¬¸ì„œë¡œ ë³€í™˜
def pdf_to_document(uploaded_file):
    temp_dir = tempfile.TemporaryDirectory()
    temp_filepath = os.path.join(temp_dir.name, uploaded_file.name)
    with open(temp_filepath, "wb") as f:
        f.write(uploaded_file.getvalue())
    loader = PyPDFLoader(temp_filepath)
    pages = loader.load_and_split()
    return pages

# íŒŒì¼ ì—…ë¡œë“œê°€ ë˜ì—ˆì„ ë•Œ ì‹¤í–‰ë˜ëŠ” ì½”ë“œ
if uploaded_file is not None:
    # í˜ì´ì§€ ë¡œë“œ
    pages = pdf_to_document(uploaded_file)
    st.write(f"í˜ì´ì§€ ìˆ˜: {len(pages)} í˜ì´ì§€ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

    # í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=300,
        chunk_overlap=20,
        length_function=len,
        is_separator_regex=False,
    )
    texts = text_splitter.split_documents(pages)
    st.write(f"ì´ {len(texts)} ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ë° ì„ë² ë”© ìƒì„±
    embeddings_model = OpenAIEmbeddings()

    # ë²¡í„° ì €ì¥ì†Œì— ì„ë² ë”©ëœ í…ìŠ¤íŠ¸ ì €ì¥
    # db = Chroma.from_documents(texts, embeddings_model)

    # Chromaë¥¼ ë©”ëª¨ë¦¬ì—ë§Œ ì €ì¥í•˜ë„ë¡ ì„¤ì •
    db = Chroma.from_documents(
        documents=texts,
        embedding=embeddings_model,
        persist_directory=".chroma_db"
        
    )
    # ì§ˆë¬¸ ì…ë ¥
    st.header("PDFì—ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”!!")
    question = st.text_input('ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”')

    # # MultiQueryRetriever ì¶”ê°€
    # retriever = MultiQueryRetriever(
    #     retriever=db.as_retriever(),
    #     llm=ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.3)  # ì˜¨ë„ë¥¼ ì•½ê°„ ë†’ì„
    # )

    retriever = MultiQueryRetriever.from_llm(
    retriever=db.as_retriever(),
    llm=ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.3)
    )

    # ì§ˆë¬¸ ë²„íŠ¼
    if st.button('ì§ˆë¬¸í•˜ê¸°'):
        with st.spinner('ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤...'):
            try:
                llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.3)
                qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever, verbose=True)
                print(qa_chain)
                result = qa_chain.invoke({"query": question})
                print(result)

                # ê²°ê³¼ ì¶œë ¥
                st.write(result.get("result", "ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. PDFì˜ ë‚´ìš©ì„ í™•ì¸í•´ ì£¼ì„¸ìš”."))
            except Exception as e:
                st.write("ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. PDF ë‚´ìš© ë° ì„¤ì •ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")
                st.write(f"ì˜¤ë¥˜ ë‚´ìš©: {e}")
=======
#ì—…ë¡œë“œ ë˜ë©´ ë™ì‘í•˜ëŠ” ì½”ë“œ
if uploaded_file is not None:
    pages = pdf_to_document(uploaded_file)

    #Split
    text_splitter = RecursiveCharacterTextSplitter(
        # Set a really small chunk size, just to show.
        chunk_size = 1000,
        chunk_overlap  = 20,
        length_function = len,
        is_separator_regex = False,
    )
    texts = text_splitter.split_documents(pages)

    #Embedding
    embeddings_model = OpenAIEmbeddings()

    # load it into Chroma
    db = Chroma.from_documents(texts, embeddings_model)

    #Question
    st.header("PDFì—ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”!!")
    question = st.text_input('ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”')

    if st.button('ì§ˆë¬¸í•˜ê¸°'):
        with st.spinner('Wait for it...'):
            llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
            qa_chain = RetrievalQA.from_chain_type(llm,retriever=db.as_retriever(), return_source_documents=True)
            result = qa_chain.invoke({"query": question})
            print(result['source_documents'])
            st.write(result["result"])



>>>>>>> a762fa9f9050eb150ae4f02cee0766e0b7025e1c
>>>>>>> dev
